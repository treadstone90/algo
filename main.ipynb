{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOlbZTRds4Fd"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pixqc/einsum-puzzles/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVzMzkmzs4Fd",
        "outputId": "27f78c0f-09c2-4394-b731-7e4ac5b7987c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Welcome to pixqc/einsum-puzzles!\n",
        "\n",
        "# Einsum (short for Einstein summation) is a powerful tool for tensor multiplication, summation, and permutation.\n",
        "# Multi-step tensor operations can be concisely expressed with a single einsum subscript.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "a, b = np.random.randn(2, 3, 4), np.random.randn(2, 3, 4)\n",
        "out1 = np.transpose(np.tensordot(a, b, axes=([2], [2])), (0, 1, 3, 2))\n",
        "out2 = np.einsum(\"ijm,lkm->ijkl\", a, b)\n",
        "print(np.allclose(out1, out2))\n",
        "# ijm,lkm->ijkl... what? This notebook attempts to demystify einsum.\n",
        "\n",
        "# We'll start with multiplying two vectors,\n",
        "# we'll end with implementing multi-head attention.\n",
        "\n",
        "# Einsum is available on numpy, torch, jax, mlx, and tinygrad.\n",
        "# We'll use numpy, but the core idea applies to all tensor library.\n",
        "\n",
        "# (Einsum cheatsheet available at the end of this notebook!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGYNHsjss4Fe",
        "outputId": "8f29e2a2-7468-40a3-ff51-555b5df8bb32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# Introduction\n",
        "\n",
        "# In einsum-land, we deal with tensor shapes.\n",
        "# We tell einsum what the output should look like,\n",
        "# and it does the operations for us.\n",
        "# np.einsum(\"INPUT->OUTPUT\", a)\n",
        "#            │      │\n",
        "#            │      └─── desired output shape\n",
        "#            └────────── a.shape\n",
        "\n",
        "# We use 'ijk' to represent dimensions.\n",
        "# When our input has one dimension,\n",
        "# we use one letter to represent that dimension.\n",
        "a = np.array([1, 2, 3])\n",
        "out = np.einsum(\"i->i\", a)  # identity function\n",
        "#                │  │\n",
        "#                │  └─── output: (3,)\n",
        "#                └────── a.shape = (3,); i = 3\n",
        "# 'i->i' means \"take the input and keep it the same\"\n",
        "print(np.allclose(a, out))\n",
        "\n",
        "# When our input has two dimensions,\n",
        "# we use two letters to represent each dimension.\n",
        "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "out = np.einsum(\"ij->ij\", a)  # identity function\n",
        "#                │   │\n",
        "#                │   └─── output: (2, 3)\n",
        "#                └─────── a.shape = (2, 3); i=2, j=3\n",
        "print(np.allclose(a, out))\n",
        "\n",
        "# Likewise with three dims.\n",
        "a = np.arange(24).reshape(2, 3, 4)\n",
        "out = np.einsum(\"ijk->ijk\", a)  # identity function\n",
        "#                │    │\n",
        "#                │    └─── output: (2, 3, 4)\n",
        "#                └──────── a.shape = (2, 3, 4); i=2, j=3, k=4\n",
        "print(np.allclose(a, out))\n",
        "\n",
        "# Notes:\n",
        "# - We can use any letter, not only 'ijk'.\n",
        "#   Ie. np.einsum('abc->abc', a) is valid.\n",
        "# - The arrow and output subscript can be ommited.\n",
        "#   Ie. np.einsum('ijk', a) is valid.\n",
        "# - This 'ijk->ijk' thing is sometimes called subscript."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "to4p-0rCs4Fe",
        "outputId": "cdf7a561-fa1f-41d8-ccc8-b462d7c47d05"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "invalid subscript '<' in einstein sum subscripts string, subscripts must be letters",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-109796248.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Compute identity of vector a.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<YOUR_ANSWER_HERE>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/einsumfunc.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspecified_out\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mc_einsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m     \u001b[0;31m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid subscript '<' in einstein sum subscripts string, subscripts must be letters"
          ]
        }
      ],
      "source": [
        "# Puzzle #1\n",
        "# Compute identity of vector a.\n",
        "a = np.arange(10)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
        "print(np.allclose(a, out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "VTTHlZvJs4Fe",
        "outputId": "6743557a-d5ce-4a87-db98-1bf44eb3d7a7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "invalid subscript '<' in einstein sum subscripts string, subscripts must be letters",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2610612583.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Compute identity of tensor a.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<YOUR_ANSWER_HERE>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/einsumfunc.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspecified_out\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mc_einsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m     \u001b[0;31m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid subscript '<' in einstein sum subscripts string, subscripts must be letters"
          ]
        }
      ],
      "source": [
        "# Puzzle #2\n",
        "# Compute identity of tensor a.\n",
        "a = np.arange(120).reshape(5, 4, 3, 2)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
        "print(np.allclose(a, out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbvRELLPs4Fe",
        "outputId": "0e9eb427-b7b0-40cd-9cf2-50c941b6c96f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Multiplication\n",
        "\n",
        "# We separate multiple inputs with comma.\n",
        "# np.einsum(\"INPUT1,INPUT2->OUTPUT\", a, b)\n",
        "#            │      │       │\n",
        "#            │      │       └─── desired output shape\n",
        "#            │      └─────────── b.shape\n",
        "#            └────────────────── a.shape\n",
        "\n",
        "# When we repeat indices in the input,\n",
        "# einsum elementwise-multiplies along that dimension.\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6])\n",
        "out = np.einsum(\"i,i->i\", a, b)\n",
        "#                │ │  │\n",
        "#                │ │  └──── output: (3,)\n",
        "#                │ │        \"keep original shape\"\n",
        "#                │ │\n",
        "#                │ └─────── b.shape = (3,); i = 3\n",
        "#                ├───────── a.shape = (3,); i = 3\n",
        "#                │\n",
        "#                ├───────── 'i' appears twice before the arrow\n",
        "#                └───────── they are elementwise multiplied\n",
        "print(np.allclose(a * b, out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "MLbaYNK0s4Fe",
        "outputId": "71b0dd35-11f5-4922-c5a0-211107e12870"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "fewer operands provided to einstein sum function than specified in the subscripts string",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-352180970.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<YOUR_ANSWER_HERE>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/einsumfunc.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspecified_out\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mc_einsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m     \u001b[0;31m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: fewer operands provided to einstein sum function than specified in the subscripts string"
          ]
        }
      ],
      "source": [
        "# Puzzle #3\n",
        "# Multiply vector a and b.\n",
        "a = np.arange(10)\n",
        "b = np.arange(10)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose(a * b, out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "x-HqiZhns4Fe",
        "outputId": "2c83cd58-4c7e-4fac-ff34-ce420af8052a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "fewer operands provided to einstein sum function than specified in the subscripts string",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1083180790.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<YOUR_ANSWER_HERE>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/einsumfunc.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspecified_out\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mc_einsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m     \u001b[0;31m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: fewer operands provided to einstein sum function than specified in the subscripts string"
          ]
        }
      ],
      "source": [
        "# Puzzle #4\n",
        "# Multiply matrix a and b.\n",
        "a = np.arange(24).reshape(2, 3, 4)\n",
        "b = np.arange(24).reshape(2, 3, 4)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose(a * b, out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdNUUHHBs4Fe"
      },
      "outputs": [],
      "source": [
        "# Summation\n",
        "\n",
        "# When we omit an index in the output,\n",
        "# we're telling einsum to sum over that dimension.\n",
        "a = np.array([1, 2, 3])\n",
        "out = np.einsum(\"i->\", a)\n",
        "#                │  │\n",
        "#                │  └─── output: scalar (all dims summed)\n",
        "#                └────── a.shape = (3,); i = 3\n",
        "print(np.allclose(np.sum(a), out))\n",
        "\n",
        "# 'ijk->' will reduce a 3d tensor into a scalar.\n",
        "a = np.arange(24).reshape(4, 3, 2)\n",
        "out = np.einsum(\"ijk->\", a)\n",
        "#                │    │\n",
        "#                │    └─── output: scalar (all dims summed)\n",
        "#                └──────── a.shape = (3,); i = 3\n",
        "print(np.allclose(np.sum(a), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVCiTVsEs4Ff"
      },
      "outputs": [],
      "source": [
        "# Puzzle #5\n",
        "# Compute sum of vector a.\n",
        "a = np.arange(10)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
        "print(np.allclose(np.sum(a), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBWulbrfs4Ff"
      },
      "outputs": [],
      "source": [
        "# Puzzle #6\n",
        "# Multiply vector a and b.\n",
        "a = np.arange(10)\n",
        "b = np.arange(10)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose(a * b, out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRgCfOEhs4Ff"
      },
      "outputs": [],
      "source": [
        "# Puzzle #7\n",
        "# Compute dot product of vector a and b.\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6])\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose(np.sum(a * b), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaBHFo5As4Ff"
      },
      "outputs": [],
      "source": [
        "# Check our understanding\n",
        "\n",
        "# Q1: 'i->i produces identity, 'i,i->i' multiplies. Why?\n",
        "# Q2: What will 'ij->ji' do to a matrix?\n",
        "# Q3: Is 'ijk' a valid input for a 2d matrix?\n",
        "# Q4: Can we elementwise add with einsum?\n",
        "\n",
        "# (Answers are available at the end of this notebook.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2psnJ8Is4Ff"
      },
      "outputs": [],
      "source": [
        "# Sum over axis\n",
        "\n",
        "# Einsum output:\n",
        "# Whatever's gone is summed over,\n",
        "# whatever's left is kept.\n",
        "\n",
        "# What we learned we can do:\n",
        "# Omit an index from the output to sum over it.\n",
        "# What we can also do:\n",
        "# Keep an index to preserve that dimension.\n",
        "a = np.arange(6).reshape(2, 3)\n",
        "out = np.einsum(\"ij->i\", a)\n",
        "#                │   │\n",
        "#                │   └─── output: (2,)\n",
        "#                │        \"sum over j, keep i\"\n",
        "#                │\n",
        "#                └─────── a.shape = (2, 3); i=2, j=3\n",
        "print(np.allclose(np.sum(a, axis=1), out))\n",
        "\n",
        "# Same thing but on axis=0\n",
        "a = np.arange(6).reshape(2, 3)\n",
        "out = np.einsum(\"ij->j\", a)\n",
        "#                │   │\n",
        "#                │   └─── output: (3,)\n",
        "#                │        \"sum over i, keep j\"\n",
        "#                │\n",
        "#                └─────── a.shape = (2, 3); i=2, j=3\n",
        "print(np.allclose(np.sum(a, axis=0), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7umNtLys4Ff"
      },
      "outputs": [],
      "source": [
        "# Puzzle #8\n",
        "# Compute np.sum(a, axis=1)\n",
        "a = np.arange(10).reshape(2, 5)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
        "print(np.allclose(np.sum(a, axis=1), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuuBvsYys4Ff"
      },
      "outputs": [],
      "source": [
        "# Puzzle #9\n",
        "# Compute np.sum(a*b, axis=0)\n",
        "a = np.arange(10).reshape(2, 5)\n",
        "b = np.arange(10).reshape(2, 5)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose(np.sum(a * b, axis=0), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SZEk-jus4Ff"
      },
      "outputs": [],
      "source": [
        "# Puzzle #10\n",
        "# Compute np.sum(a, axis=1)\n",
        "a = np.arange(24).reshape(2, 3, 4)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
        "print(np.allclose(np.sum(a, axis=1), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "lR856dI2s4Ff",
        "outputId": "576c614a-7bed-478c-8970-0ca6c65a42b5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3962119772.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Diagonal: we use the same index for both dimensions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ii->i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#                │   │\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "# Diagonal and trace\n",
        "\n",
        "# Diagonal: we use the same index for both dimensions.\n",
        "a = np.arange(9).reshape(3, 3)\n",
        "out = np.einsum(\"ii->i\", a)\n",
        "#                │   │\n",
        "#                │   └─── output: (3,)\n",
        "#                └─────── a.shape = (3, 3); i=3\n",
        "# 'ii' is saying: \"get items where row and column is the same\".\n",
        "print(np.allclose(np.diag(a), out))\n",
        "\n",
        "# Trace: it's diagonal but the output is summed.\n",
        "a = np.arange(9).reshape(3, 3)\n",
        "out = np.einsum(\"ii->\", a)\n",
        "#                │   │\n",
        "#                │   └─── output: scalar\n",
        "#                └─────── a.shape = (3, 3); i=3\n",
        "print(np.allclose(np.trace(a), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOkUBqOms4Ff"
      },
      "outputs": [],
      "source": [
        "# Puzzle #11\n",
        "# Compute np.diag(a)\n",
        "a = np.arange(9).reshape(3, 3)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
        "print(np.allclose(np.diag(a), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTKMaTags4Ff"
      },
      "outputs": [],
      "source": [
        "# Puzzle #12\n",
        "# Compute np.trace(a)\n",
        "a = np.arange(9).reshape(3, 3)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
        "print(np.allclose(np.trace(a), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8zyySmRs4Ff"
      },
      "outputs": [],
      "source": [
        "# Broadcasting\n",
        "\n",
        "# We can use einsum to broadcast.\n",
        "\n",
        "# Let’s do a quick refresher on tensor broadcasting:\n",
        "# - Right-align shapes, prepend 1s if needed.\n",
        "# - Dims are compatible if equal or 1.\n",
        "# - 1s are broadcast to match the other size.\n",
        "# - Result shape is max size for each dimension.\n",
        "\n",
        "# Broadcasting along an axis\n",
        "a = np.arange(6).reshape(2, 3)\n",
        "b = np.arange(3)\n",
        "out = np.einsum(\"ij,j->ij\", a, b)\n",
        "#                │  │  │\n",
        "#                │  │  └─── output: (2, 3)\n",
        "#                │  │       'i' and 'j' are kept\n",
        "#                │  │\n",
        "#                │  └────── b.shape = (3,); j=3\n",
        "#                └───────── a.shape = (2, 3); i=2, j=3\n",
        "# One way to look at it:\n",
        "# 'ij,j->ij' is broadcasted to 'ij,ij->ij' by einsum.\n",
        "print(np.allclose(a * b, out))\n",
        "\n",
        "# Broadcasting with higher dimensions\n",
        "a = np.arange(24).reshape(2, 3, 4)\n",
        "b = np.arange(3)\n",
        "out = np.einsum(\"ijk,j->ijk\", a, b)\n",
        "#                │   │     │\n",
        "#                │   │     └─── output: (2, 3, 4)\n",
        "#                │   │          'i', 'j', and 'k' are kept\n",
        "#                │   │\n",
        "#                │   └───────── b.shape = (3, 1); j=3, ...=(1,)\n",
        "#                └───────────── a.shape = (2, 3, 4); i=2, j=3, k=4\n",
        "# Likewise: 'ijk,j->ijk' is broadcasted to 'ijk,ijk->ijk'.\n",
        "print(np.allclose(a * b[:, None], out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1j8h_Pfms4Ff"
      },
      "outputs": [],
      "source": [
        "# Puzzle #13\n",
        "# Compute np.sum(a*b,axis=1)\n",
        "# b must be broadcasted.\n",
        "a = np.arange(6).reshape(2, 3)\n",
        "b = np.arange(3)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose(np.sum(a * b, axis=1), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6yzV57Ps4Ff"
      },
      "outputs": [],
      "source": [
        "# Puzzle #14\n",
        "# Compute np.allclose(np.sum(a*b, axis=(1, 2))\n",
        "# b must be broadcasted.\n",
        "a = np.arange(24).reshape(2, 3, 4)\n",
        "b = np.arange(12).reshape(3, 4)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose(np.sum(a * b, axis=(1, 2)), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhV-MR18s4Ff"
      },
      "outputs": [],
      "source": [
        "# Outer product\n",
        "\n",
        "# When we use different indices for each input,\n",
        "# and combine them all in the output,\n",
        "# we're telling einsum to compute the outer product.\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6, 7])\n",
        "out = np.einsum(\"i,j->ij\", a, b)\n",
        "#                │ │  │\n",
        "#                │ │  │\n",
        "#                │ │  └──── output: (3, 4)\n",
        "#                │ │        we create all combinations of i and j\n",
        "#                │ │\n",
        "#                │ └─────── b.shape = (4,); j = 4\n",
        "#                └───────── a.shape = (3,); i = 3\n",
        "print(np.allclose(np.outer(a, b), out))\n",
        "\n",
        "# This creates a 4d array by combining all elements of both input.\n",
        "a = np.arange(6).reshape(2, 3)\n",
        "b = np.arange(8).reshape(2, 4)\n",
        "out = np.einsum(\"ij,kl->ijkl\", a, b)\n",
        "#                │  │   │\n",
        "#                │  │   │\n",
        "#                │  │   └── output: (2, 3, 2, 4)\n",
        "#                │  │       we create all combinations of i, j, k, l\n",
        "#                │  │\n",
        "#                │  └────── b.shape = (2, 4); k = 2, l = 4\n",
        "#                └───────── a.shape = (2, 3); i = 2, j = 3\n",
        "print(np.allclose(np.outer(a, b).reshape(2, 3, 2, 4), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSZZFDrTs4Ff"
      },
      "outputs": [],
      "source": [
        "# Puzzle #15\n",
        "# Compute the outer product of vectors a and b\n",
        "a = np.arange(3)\n",
        "b = np.arange(4)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose(np.outer(a, b), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRLv6ZTks4Fg"
      },
      "outputs": [],
      "source": [
        "# Puzzle #16\n",
        "# Compute the outer product of vector a and matrix b.\n",
        "a = np.arange(3)\n",
        "b = np.arange(4).reshape(2, 2)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose(np.outer(a, b.flatten()).reshape(3, 2, 2), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blOL2st_s4Fg"
      },
      "outputs": [],
      "source": [
        "# Transposition and permutation\n",
        "\n",
        "# This is where einsum shines.\n",
        "# Einsum permutations are self-documenting.\n",
        "\n",
        "# Transpose: we swap the order of indices in the output.\n",
        "a = np.arange(6).reshape(2, 3)\n",
        "out = np.einsum(\"ij->ji\", a)\n",
        "#                │   │\n",
        "#                │   └─── output: (3, 2)\n",
        "#                │        we swap i and j\n",
        "#                │\n",
        "#                └─────── a.shape = (2, 3); i=2, j=3\n",
        "print(np.allclose(a.T, out))\n",
        "\n",
        "# Permute: we can rearrange dimensions in any order we want.\n",
        "a = np.arange(24).reshape(2, 3, 4)\n",
        "out = np.einsum(\"ijk->kji\", a)\n",
        "#                │    │\n",
        "#                │    └─── output: (4, 3, 2)\n",
        "#                │         we reverse the order of indices\n",
        "#                │\n",
        "#                └──────── a.shape = (2, 3, 4); i=2, j=3, k=4\n",
        "print(np.allclose(np.transpose(a, (2, 1, 0)), out))\n",
        "\n",
        "# Even with higher dimensions, it's clear what we're doing.\n",
        "a = np.arange(120).reshape(2, 3, 4, 5)\n",
        "out = np.einsum(\"ijkl->jilk\", a)\n",
        "#                │     │\n",
        "#                │     └─── output: (3, 2, 5, 4)\n",
        "#                │          we swap i-j and k-l\n",
        "#                │\n",
        "#                └───────── a.shape = (2, 3, 4, 5); i=2, j=3, k=4, l=5\n",
        "print(np.allclose(np.transpose(a, (1, 0, 3, 2)), out))\n",
        "\n",
        "# We can multiply elementwise then transpose.\n",
        "a = np.arange(6).reshape(2, 3)\n",
        "b = np.arange(6).reshape(2, 3)\n",
        "out = np.einsum(\"ij,ij->ji\", a, b)\n",
        "#                │  │   │\n",
        "#                │  │   └─── output: (3, 2)\n",
        "#                │  │        we multiply elementwise, then transpose\n",
        "#                │  │\n",
        "#                │  └─────── b.shape = (2, 3); i=2, j=3\n",
        "#                └────────── a.shape = (2, 3); i=2, j=3\n",
        "print(np.allclose((a * b).T, out))\n",
        "\n",
        "# And even permute.\n",
        "a = np.arange(24).reshape(2, 3, 4)\n",
        "b = np.arange(24).reshape(2, 3, 4)\n",
        "out = np.einsum(\"ijk,ijk->kji\", a, b)\n",
        "#                │   │     │\n",
        "#                │   │     └─ output: (4, 3, 2)\n",
        "#                │   │        we multiply elementwise, then rearrange\n",
        "#                │   │\n",
        "#                │   └─────── b.shape = (2, 3, 4); i=2, j=3, k=4\n",
        "#                └─────────── a.shape = (2, 3, 4); i=2, j=3, k=4\n",
        "print(np.allclose((a * b).transpose(2, 1, 0), out))\n",
        "\n",
        "# Note: when multiplying and permuting,\n",
        "# the multiplication takes precedence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnHlX31us4Fg"
      },
      "outputs": [],
      "source": [
        "# Puzzle #17\n",
        "# Transpose the 2d matrix a.\n",
        "a = np.arange(6).reshape(2, 3)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
        "print(np.allclose(a.T, out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfrr8EPYs4Fg"
      },
      "outputs": [],
      "source": [
        "# Puzzle #18\n",
        "# Permute the tensor from shape (2,3,4) to (4,3,2)\n",
        "a = np.arange(24).reshape(2, 3, 4)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
        "out.shape\n",
        "print(np.allclose(np.transpose(a, (2, 1, 0)), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P4__v0Is4Fg"
      },
      "outputs": [],
      "source": [
        "# Puzzle #19\n",
        "# Permute the tensor from shape (2,3,4,5) to (5,3,2,4)\n",
        "a = np.arange(120).reshape(2, 3, 4, 5)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
        "print(np.allclose(np.transpose(a, (3, 1, 0, 2)), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vbat_txis4Fg"
      },
      "outputs": [],
      "source": [
        "# Puzzle #20\n",
        "# Multiply a and b elementwise, then permute to (4,2,3)\n",
        "a = np.arange(24).reshape(2, 3, 4)\n",
        "b = np.arange(24).reshape(2, 3, 4)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose((a * b).transpose(2, 0, 1), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgLk6hFOs4Fg"
      },
      "outputs": [],
      "source": [
        "# Matrix multiplication\n",
        "\n",
        "# Repeat on input to multiply, omit from ouptut to sum.\n",
        "# Combine both we get matrix multiplication.\n",
        "a = np.arange(6).reshape(3, 2)\n",
        "b = np.arange(6).reshape(2, 3)\n",
        "out = np.einsum(\"ij,jk->ik\", a, b)\n",
        "#                │  │   │\n",
        "#                │  │   ├──── output: (3, 3)\n",
        "#                │  │   │     i and k remain, j is summed over\n",
        "#                │  │   │\n",
        "#                │  └───┼──── second input: b.shape == (2, 3)\n",
        "#                │      │     j = 2; k = 3\n",
        "#                │      │\n",
        "#                ├──────┼──── first input: a.shape == (3, 2)\n",
        "#                │      │     i = 3; j = 2\n",
        "#                │      │\n",
        "#                ├──────┼──── 'j' appears twice on input\n",
        "#                └──────┼──── they are elementwise multiplied\n",
        "#                       │\n",
        "#                       ├──── 'j' is omitted from output\n",
        "#                       └──── they are summed over\n",
        "print(np.allclose(a @ b, out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5uFQs3Xs4Fg"
      },
      "outputs": [],
      "source": [
        "# Puzzle #21\n",
        "# Compute a @ b\n",
        "a = np.arange(6).reshape(3, 2)\n",
        "b = np.arange(6).reshape(2, 3)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose(a @ b, out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJJ2v3jfs4Fg"
      },
      "outputs": [],
      "source": [
        "# Puzzle #22\n",
        "# Compute (a @ b).T\n",
        "a = np.arange(6).reshape(3, 2)\n",
        "b = np.arange(6).reshape(2, 3)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose((a @ b).T, out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBuqsBNds4Fg"
      },
      "outputs": [],
      "source": [
        "# Puzzle #23\n",
        "# Compute a @ b.T\n",
        "a = np.arange(6).reshape(3, 2)\n",
        "b = np.arange(6).reshape(3, 2)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose(a @ b.T, out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXwwy82_s4Fg"
      },
      "outputs": [],
      "source": [
        "# Puzzle #24\n",
        "# Compute a @ b\n",
        "a = np.arange(24).reshape(2, 3, 4)\n",
        "b = np.arange(20).reshape(4, 5)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose(a @ b, out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h3AmdMps4Fg"
      },
      "outputs": [],
      "source": [
        "# Puzzle #25\n",
        "# Compute a @ b (batched matmul)\n",
        "a = np.random.randn(10, 3, 4)\n",
        "b = np.random.randn(10, 4, 5)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose(a @ b, out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUGGPoWds4Fg"
      },
      "outputs": [],
      "source": [
        "# Puzzle #26\n",
        "# Compute np.tensordot(a,b,axes=([1],[0]))\n",
        "# The output should be (3, 4, 6)\n",
        "a = np.random.randn(3, 5, 4)\n",
        "b = np.random.randn(5, 6)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose(np.tensordot(a, b, axes=([1], [0])), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm4rXRzAs4Fg"
      },
      "outputs": [],
      "source": [
        "# Puzzle #27\n",
        "# Compute np.tensordot(a,b,axes=([0],[1])).transpose(1,0,2)\n",
        "# The output should be (4, 3, 6)\n",
        "a = np.random.randn(5, 3, 4)\n",
        "b = np.random.randn(6, 5)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose(np.tensordot(a, b, axes=([0], [1])).transpose(1, 0, 2), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErtahI2Us4Fk"
      },
      "outputs": [],
      "source": [
        "# Puzzle #28\n",
        "# Compute np.tensordot(a,b,axes=([1],[0])).sum(axis=(1,2)).T\n",
        "# The output should be (6, 2)\n",
        "a = np.random.randn(2, 3, 4)\n",
        "b = np.random.randn(3, 5, 6)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose(np.tensordot(a, b, axes=([1], [0])).sum(axis=(1, 2)).T, out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSGlBr6Js4Fk"
      },
      "outputs": [],
      "source": [
        "# Check our understanding\n",
        "\n",
        "# Q5: Let a=(3,2); b=(3,2), why is a@b.T expressed as 'ij,kj->ik'?\n",
        "# Q6: Let a=(3,2); b=(3,2), what's the difference between\n",
        "#     np.einsum('ij,jk->ik', a, b.T) and np.einsum('ij,kj->ik', a, b)?\n",
        "\n",
        "# (Answers are available at the end of this notebook.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F35L1bKWs4Fk"
      },
      "outputs": [],
      "source": [
        "# Arrowless subscript\n",
        "\n",
        "# np.einsum('ijk,ijk->', a, b) is a valid einsum.\n",
        "# np.einsum('ijk,ijk', a, b) is also a valid einsum.\n",
        "\n",
        "# When we omit arrow and output subscript from einsum:\n",
        "# - Repeated indices across inputs are multiplied.\n",
        "# - Non-repeating indices remain in the output.\n",
        "# - For a single input, it acts as an identity operation.\n",
        "\n",
        "# Single input: the identity operation.\n",
        "a = np.arange(6).reshape(2, 3)\n",
        "out = np.einsum(\"ij\", a)\n",
        "#                │\n",
        "#                └─────── a.shape = (2, 3); i=2, j=3\n",
        "#                         no repeated indices, nothing to multiply\n",
        "#                         both 'i' and 'j' remain in output\n",
        "print(np.allclose(np.einsum(\"ij->ij\", a), out))\n",
        "\n",
        "# Multiple inputs, repeated indices.\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6])\n",
        "out = np.einsum(\"i,i\", a, b)\n",
        "#                │ │\n",
        "#                │ └─────── b.shape = (3,); i=3\n",
        "#                └───────── a.shape = (3,); i=3\n",
        "#                           'i' repeated across inputs, so it's multiplied\n",
        "#                           no indices remain, so output is scalar\n",
        "print(np.allclose(np.einsum(\"i,i->\", a, b), out))\n",
        "\n",
        "# Matrix multiplication.\n",
        "a = np.arange(6).reshape(2, 3)\n",
        "b = np.arange(6).reshape(3, 2)\n",
        "out = np.einsum(\"ij,jk\", a, b)\n",
        "#                │  │\n",
        "#                │  └─────── b.shape = (3, 2); j=3, k=2\n",
        "#                └────────── a.shape = (2, 3); i=2, j=3\n",
        "#                            'j' repeated across inputs, so it's multiplied\n",
        "#                            'i' and 'k' remain, forming output shape\n",
        "print(np.allclose(np.einsum(\"ij,jk->ik\", a, b), out))\n",
        "\n",
        "# Multiple inputs, no repeated indices: outer product.\n",
        "a = np.array([1, 2])\n",
        "b = np.array([3, 4, 5])\n",
        "out = np.einsum(\"i,j\", a, b)\n",
        "#                │ │\n",
        "#                │ └─────── b.shape = (3,); j=3\n",
        "#                └───────── a.shape = (2,); i=2\n",
        "#                           no repeated indices, no multiply\n",
        "#                           both 'i' and 'j' remain, forming output shape\n",
        "print(np.allclose(np.einsum(\"i,j->ij\", a, b), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIwkcMA5s4Fk"
      },
      "outputs": [],
      "source": [
        "# Ellipsis\n",
        "\n",
        "# We can use '...' to represent leftover dimension.\n",
        "# It's like a placeholder for indices.\n",
        "# Say we have 'ijkl', and we're only interested in 'kl'.\n",
        "# Doing '...kl' means the '...' represents 'ij'.\n",
        "\n",
        "# Transposing [:-2].\n",
        "a = np.random.randn(2, 3, 4, 5)\n",
        "out = np.einsum(\"...ij->...ji\", a)\n",
        "#                 │ │    │  │\n",
        "#                 │ │    │  └─── output: (2, 3, 5, 4)\n",
        "#                 │ │    │       we swap i and j at the end\n",
        "#                 │ │    └────── '...' = (2, 3)\n",
        "#                 │ │\n",
        "#                 │ └─────────── a.shape[-2:] = (4, 5); i=4, j=5\n",
        "#                 └───────────── a.shape[:-2] = '...'=(2, 3)\n",
        "print(np.allclose(a.transpose(0, 1, 3, 2), out))\n",
        "\n",
        "# Transposing head and tail.\n",
        "a = np.random.randn(2, 3, 4, 5)\n",
        "out = np.einsum(\"i...k->k...i\", a)\n",
        "#                │ │ │  │ │ │\n",
        "#                │ │ │  │ │ └─── output[0] = a.shape[3] = 5\n",
        "#                │ │ │  │ └───── '...' = (3, 4)\n",
        "#                │ │ │  └─────── output[3] = a.shape[0] = 2\n",
        "#                │ │ └────────── a.shape[-1] = 5\n",
        "#                │ └──────────── a.shape[1:3] = '...' = (3, 4)\n",
        "#                └────────────── a.shape[0] = 2\n",
        "# It's like saying:\n",
        "# We only care about 'ik' dim, the '...' represents (3,4).\n",
        "# When we use '...' on output, it means (3,4).\n",
        "print(np.allclose(a.transpose(3, 1, 2, 0), out))\n",
        "\n",
        "# Batched matmul.\n",
        "a = np.random.randn(2, 3, 4, 5)\n",
        "b = np.random.randn(5, 6)\n",
        "out = np.einsum(\"...j,jk->...k\", a, b)\n",
        "#                 │ │ │    │ │\n",
        "#                 │ │ │    │ └─── output: (2, 3, 4, 6); k=6\n",
        "#                 │ │ │    └───── '...' = (2, 3, 4)\n",
        "#                 │ │ │\n",
        "#                 │ │ └────────── b = (5, 6); j=5, k=6\n",
        "#                 │ └──────────── a[-1] = 5; j=5\n",
        "#                 └────────────── a[:-1] = (2, 3, 4); ...=(2, 3, 4)\n",
        "print(np.allclose(a @ b, out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfKC-I6is4Fk"
      },
      "outputs": [],
      "source": [
        "# Puzzle #29\n",
        "# Transpose the last two dimension of tensor a.\n",
        "a = np.random.randn(2, 3, 4, 5, 6, 7)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a)\n",
        "print(np.allclose(a.transpose(0, 1, 2, 3, 5, 4), out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqzJqOgJs4Fk"
      },
      "outputs": [],
      "source": [
        "# Puzzle #30\n",
        "a = np.random.randn(3, 4, 5)\n",
        "b = np.random.randn(5, 6)\n",
        "out = np.einsum(\"<YOUR_ANSWER_HERE>\", a, b)\n",
        "print(np.allclose(a @ b, out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_Xzl2qNs4Fk"
      },
      "outputs": [],
      "source": [
        "# Check our understanding\n",
        "\n",
        "# Q7: What are the pros/cons of arrowless subscript?\n",
        "# Q8: When is ellipsis useful? When should we use it vs. not use it?\n",
        "\n",
        "# (Answers are available at the end of this notebook.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjY7gi1js4Fk"
      },
      "outputs": [],
      "source": [
        "# Attention... is all we need.\n",
        "# Let's implment an attention head with einsum!\n",
        "\n",
        "# B: batch size\n",
        "# L: sequence length\n",
        "# D: model dimension\n",
        "# H: number of attention heads in a layer\n",
        "# K: size of each attention key or value\n",
        "b, l, d, h, k = 16, 10, 32, 4, 8\n",
        "# In Karpathy's/llm.c's lingo: BLD = BTC\n",
        "\n",
        "\n",
        "def softmax(x, axis=-1):\n",
        "  e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "  return e_x / e_x.sum(axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "w_q_dk = np.random.randn(d, k)\n",
        "w_k_dk = np.random.randn(d, k)\n",
        "w_v_dk = np.random.randn(d, k)\n",
        "mask = np.where(np.tril(np.ones((l, l))) == 1, 0.0, -np.inf)\n",
        "\n",
        "\n",
        "def head_np(input_bld):\n",
        "  q_blk = input_bld @ w_q_dk\n",
        "  k_blk = input_bld @ w_k_dk\n",
        "  v_blk = input_bld @ w_v_dk\n",
        "  sqrt_k = np.sqrt(k_blk.shape[-1])\n",
        "  scores_bll = (q_blk @ k_blk.transpose(0, 2, 1)) / sqrt_k + mask\n",
        "  attention_weights_bll = softmax(scores_bll, axis=-1)\n",
        "  out_blk = attention_weights_bll @ v_blk\n",
        "  return out_blk\n",
        "\n",
        "\n",
        "# Puzzle #31\n",
        "def head(input_bld):\n",
        "  # <YOUR_ANSWER_HERE> implement a single attention head with einsum.\n",
        "  pass\n",
        "\n",
        "\n",
        "input_bld = np.random.randn(b, l, d)\n",
        "ho = head(input_bld)\n",
        "ho_np = head_np(input_bld)\n",
        "print(np.allclose(ho, ho_np))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJjoCAqhs4Fk"
      },
      "outputs": [],
      "source": [
        "# Attention... is all we need... part 2.\n",
        "# Let's implment multi-head attention in einsum!\n",
        "\n",
        "\n",
        "# B: batch size\n",
        "# L: sequence length\n",
        "# D: model dimension\n",
        "# H: number of attention heads in a layer\n",
        "# K: size of each attention key or value\n",
        "b, l, d, h, k = 16, 10, 32, 4, 8\n",
        "# In Karpathy's/llm.c's lingo: BLD = BTC\n",
        "\n",
        "\n",
        "w_q_dhk = np.random.randn(d, h, k)\n",
        "w_k_dhk = np.random.randn(d, h, k)\n",
        "w_v_dhk = np.random.randn(d, h, k)\n",
        "w_o_hkd = np.random.randn(h, k, d)\n",
        "\n",
        "mask = np.where(np.tril(np.ones((l, l))) == 1, 0.0, -np.inf)\n",
        "\n",
        "\n",
        "def softmax(x, axis=-1):\n",
        "  e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "  return e_x / e_x.sum(axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "def attention_np(input_bld):\n",
        "  q_blhk = np.dot(input_bld, w_q_dhk.reshape(d, h * k)).reshape(b, l, h, k)\n",
        "  k_blhk = np.dot(input_bld, w_k_dhk.reshape(d, h * k)).reshape(b, l, h, k)\n",
        "  v_blhk = np.dot(input_bld, w_v_dhk.reshape(d, h * k)).reshape(b, l, h, k)\n",
        "  q_bhlk = q_blhk.transpose(0, 2, 1, 3)\n",
        "  k_bhkl = k_blhk.transpose(0, 2, 3, 1)\n",
        "  v_bhlk = v_blhk.transpose(0, 2, 1, 3)\n",
        "  scores_bhll = np.matmul(q_bhlk, k_bhkl) / np.sqrt(k)\n",
        "  scores_bhll = softmax(scores_bhll + mask, axis=-1)\n",
        "  out_bhlk = np.matmul(scores_bhll, v_bhlk)\n",
        "  out_blhk = out_bhlk.transpose(0, 2, 1, 3)\n",
        "  out_bld = np.dot(out_blhk.reshape(b, l, h * k), w_o_hkd.reshape(h * k, d))\n",
        "  return out_bld\n",
        "\n",
        "\n",
        "# Puzzle #32\n",
        "def attention(input_bld):\n",
        "  # <YOUR_ANSWER_HERE> implement multi-head attention with einsum.\n",
        "  pass\n",
        "\n",
        "\n",
        "input_bld = np.random.randn(b, l, d)\n",
        "output_np = attention_np(input_bld)\n",
        "output_einsum = attention(input_bld)\n",
        "print(np.allclose(output_np, output_einsum))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJrdIrcds4Fk"
      },
      "outputs": [],
      "source": [
        "# Check our understanding: answered\n",
        "\n",
        "# Q1A: 'i,i->i' has repeated 'i' on inputs, they're multiplied.\n",
        "#      'i->i' has no repeated inputs.\n",
        "# Q2A: 'ij->ji' will transpose the matrix.\n",
        "# Q3A: 'ijk' is not a valid subscript against 2d matrix.\n",
        "# Q4A: no, the only elementwise operation supported by einsum is multiply.\n",
        "# Q5A: 'ij,kj->ik' performs a@b.T because the tensor being dotted over\n",
        "#      is at shape[1] for both matrix, and they're denoted by 'j'. It doens't\n",
        "#      matter where the mul+sum happens. 'ik' remains after matmul.\n",
        "# Q6A: np.einsum('ij,jk->ik', a, b.T) transposes b before doing matmul.\n",
        "#      np.einsum('ij,kj->ik', a, b) handles the transposition for us.\n",
        "# Q7A: pros of arrowless subscript: shorter; cons: can't transpose.\n",
        "# Q8A: ellipsis can be useful when working with batches of high-dim tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn6-ReGJs4Fk"
      },
      "outputs": [],
      "source": [
        "# Einsum cheatsheet\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def compare(einsum_expr, np_fn, *xs):\n",
        "  out = np.einsum(einsum_expr, *xs)\n",
        "  out_np = np_fn(*xs)\n",
        "  return np.allclose(out, out_np)\n",
        "\n",
        "\n",
        "# Vector operations\n",
        "xs = np.random.randn(3)\n",
        "compare(\"i\", lambda x: x, xs)\n",
        "compare(\"i->\", lambda x: np.sum(x), xs)\n",
        "\n",
        "# Matrix operations\n",
        "xs = np.random.randn(3, 3)\n",
        "compare(\"ij->ji\", lambda x: x.T, xs)\n",
        "compare(\"ij->\", lambda x: np.sum(x), xs)\n",
        "compare(\"ii->i\", lambda x: np.diag(x), xs)\n",
        "compare(\"ii->\", lambda x: np.trace(x), xs)\n",
        "compare(\"ij->j\", lambda x: np.sum(x, axis=0), xs)\n",
        "compare(\"ij->i\", lambda x: np.sum(x, axis=1), xs)\n",
        "\n",
        "# Elementwise operations with two vectors\n",
        "xs = np.random.randn(3), np.random.randn(3)\n",
        "compare(\"i,i->i\", lambda x1, x2: x1 * x2, *xs)\n",
        "compare(\"i,i->\", lambda x1, x2: np.sum(x1 * x2), *xs)\n",
        "compare(\"i,j->ij\", lambda x1, x2: np.outer(x1, x2), *xs)\n",
        "compare(\"i,j->ji\", lambda x1, x2: np.outer(x1, x2).T, *xs)\n",
        "\n",
        "# Elementwise operations with two matrices\n",
        "xs = np.random.randn(2, 3, 4), np.random.randn(2, 3, 4)\n",
        "compare(\"ijk,ijk->ijk\", lambda x1, x2: x1 * x2, *xs)\n",
        "compare(\"ijk,ijk->\", lambda x1, x2: np.sum(x1 * x2), *xs)\n",
        "compare(\"ijk,ijk->ij\", lambda x1, x2: np.sum(x1 * x2, axis=2), *xs)\n",
        "compare(\"ijk,ijk->k\", lambda x1, x2: np.sum(x1 * x2, axis=(0, 1)), *xs)\n",
        "\n",
        "# Broadcasting vector to matrix\n",
        "xs = np.random.randn(3, 4), np.random.randn(4)\n",
        "compare(\"ij,j->ij\", lambda x1, x2: x1 * x2[:, None].T, *xs)\n",
        "compare(\"ij,j->i\", lambda x1, x2: np.sum(x1 * x2, axis=1), *xs)\n",
        "\n",
        "# Broadcasting matrix to 3d tensor\n",
        "xs = np.random.randn(2, 3, 4), np.random.randn(3, 4)\n",
        "compare(\"ijk,jk->ijk\", lambda x1, x2: x1 * x2, *xs)\n",
        "compare(\"ijk,jk->ik\", lambda x1, x2: np.sum(x1 * x2, axis=1), *xs)\n",
        "\n",
        "# Basic ellipsis operations\n",
        "xs = np.random.randn(2, 3, 4, 5)\n",
        "compare(\"...ij->...ji\", lambda x: np.swapaxes(x, -2, -1), xs)\n",
        "compare(\"i...->...\", lambda x: np.sum(x, axis=0), xs)\n",
        "compare(\"ij...->...ij\", lambda x: np.moveaxis(x, [0, 1], [-2, -1]), xs)\n",
        "\n",
        "# Ellipsis with broadcasting\n",
        "xs = np.random.randn(2, 3, 4, 5), np.random.randn(4, 5)\n",
        "compare(\"...ij,ij->...i\", lambda x1, x2: np.sum(x1 * x2, axis=-1), *xs)\n",
        "\n",
        "# Element-wise operations and reductions\n",
        "xs = np.random.randn(3, 2), np.random.randn(3, 2)\n",
        "compare(\"ij,ij->ij\", lambda x1, x2: x1 * x2, *xs)\n",
        "compare(\"ij,ij->\", lambda x1, x2: np.sum(x1 * x2), *xs)\n",
        "compare(\"ij,ij->j\", lambda x1, x2: np.sum(x1 * x2, axis=0), *xs)\n",
        "compare(\"ij,ij->i\", lambda x1, x2: np.sum(x1 * x2, axis=1), *xs)\n",
        "\n",
        "# Matrix multiplications\n",
        "xs = np.random.randn(3, 2), np.random.randn(3, 2)\n",
        "compare(\"ki,kj->ij\", lambda x1, x2: x1.T @ x2, *xs)\n",
        "compare(\"ik,jk->ij\", lambda x1, x2: x1 @ x2.T, *xs)\n",
        "compare(\"ik,jk->\", lambda x1, x2: np.sum(x1 @ x2.T), *xs)\n",
        "compare(\"ik,jk->j\", lambda x1, x2: np.sum(x1 @ x2.T, axis=0), *xs)\n",
        "compare(\"ik,jk->i\", lambda x1, x2: np.sum(x1 @ x2.T, axis=1), *xs)\n",
        "\n",
        "# Combining summation, multiplication, transposition\n",
        "xs = np.random.randn(2, 3, 4), np.random.randn(2, 3, 4)\n",
        "compare(\"ijk,ijk->ijk\", lambda x1, x2: x1 * x2, *xs)\n",
        "compare(\"ijk,ijk->\", lambda x1, x2: np.sum(x1 * x2), *xs)\n",
        "compare(\"ijk,ijk->ij\", lambda x1, x2: np.sum(x1 * x2, axis=2), *xs)\n",
        "compare(\"ijk,ijk->k\", lambda x1, x2: np.sum(x1 * x2, axis=(0, 1)), *xs)\n",
        "compare(\"ikl,jkl->ij\", lambda x1, x2: np.tensordot(x1, x2, axes=([1, 2], [1, 2])), *xs)\n",
        "compare(\"ijm,lkm->\", lambda x1, x2: np.sum(np.tensordot(x1, x2, axes=([2], [2]))), *xs)\n",
        "compare(\"ikl,jkl->j\", lambda x1, x2: np.sum(np.tensordot(x1, x2, axes=([1, 2], [1, 2])),axis=0), *xs)  # fmt: skip\n",
        "compare(\"ijm,lkm->ijkl\", lambda x1, x2: np.transpose(np.tensordot(x1, x2, axes=([2], [2])), (0, 1, 3, 2)), *xs)  # fmt: skip\n",
        "compare(\"ijm,lkm->il\", lambda x1, x2: np.sum(np.transpose(np.tensordot(x1, x2, axes=([2], [2])), (0, 1, 3, 2)), axis=(1,2)), *xs)  # fmt: skip"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}